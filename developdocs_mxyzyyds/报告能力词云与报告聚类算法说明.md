# 报告聚类与报告能力关键词算法说明

> 孟剑卫主编、余东骏修订

## 参考资料

1. J. Wang et al., "Characterizing Crowds to Better Optimize Worker Recommendation in Crowdsourced Testing," in IEEE Transactions on Software Engineering, vol. 47, no. 6, pp. 1259-1276, 1 June 2021, doi: 10.1109/TSE.2019.2918520.
2. HanLP官方仓库：https://github.com/hankcs/HanLP

## 报告聚类

### 基本概念

#### 文本聚类

**文本聚类**指的是对文档进行聚类分析，被广泛用于文本挖掘和信息检索领域。

文本聚类的基本流程分为特征提取和向量聚类两步， 如果能将文档表示为向量，就可以对其应用聚类算法。这种表示过程称为**特征提取**。

#### 词袋模型

**词袋**(bag-of-words )是信息检索与自然语言处理中最常用的文档表示模型，它将文档想象为一个装有词语的袋子， 通过袋子中每种词语的计数等统计量将文档表示为向量。一般选取训练集文档的所有词语构成一个词表，词表之外的词语称为 OOV，不予考虑。一旦词表固定下来，假设大小为 N。则任何一个文档都可以通过这种方法转换为一个N维向量。词袋模型不考虑词序。

#### K-Means算法

形式化定义 k均值算法所解决的问题。给定 n 个向量 $d_{1}$ 到 $d_{n}$，以及一个整数 k，要求找出 k 个簇 $S_{1}$ 到 $S_{k}$ 以及各自的质心 $C_{1}$ 到 $C_{k}$，使得下式最小。

![image-20220528020831469](https://ydjsir-edu.oss-cn-shanghai.aliyuncs.com/SE3/pictures/image-20220528020831469.png)

其中 ||di - Cr|| 是向量与质心的欧拉距离，|(Euclidean) 称作聚类的**准则函数**。也就是说，k均值以最小化每个向量到质心的欧拉距离的平方和为准则进行聚类，所以该准则函数有时也称作**平方误差和**函数。

而质心的计算就是簇内数据点的几何平均。

![image-20220528021218871](https://ydjsir-edu.oss-cn-shanghai.aliyuncs.com/SE3/pictures/image-20220528021218871.png)

其中，$s_{i}$ 是簇 Si 内所有向量之和，称作**合成向量**。

生成 k 个簇的 k均值算法是一种迭代式算法，每次迭代都在上一步的基础上优化聚类结果，步骤如下:

- 选取 k 个点作为 k 个簇的初始质心。
- 将所有点分别分配给最近的质心所在的簇。
- 重新计算每个簇的质心。
- 重复步骤 2 和步骤 3 直到质心不再发生变化。

k均值算法虽然无法保证收敛到全局最优，但能够有效地收敛到一个局部最优点。

#### 重复二分聚类

**重复二分聚类**(repeated bisection clustering) 是 k均值算法的效率加强版，其名称中的bisection是“二分”的意思，指的是反复对子集进行二分。该算法的步骤如下:

- 挑选一个簇进行划分。
- 利用 k均值算法将该簇划分为 2 个子集。
- 重复步骤 1 和步骤 2，直到产生足够舒朗的簇。

每次产生的簇由上到下形成了一颗二叉树结构。

正是由于这个性质，重复二分聚类算得上一种基于划分的层次聚类算法。如果我们把算法运行的中间结果存储起来，就能输出一棵具有层级关系的树。树上每个节点都是一个簇，父子节点对应的簇满足包含关系。虽然每次划分都基于 k均值，由于每次二分都仅仅在一个子集上进行，输人数据少，算法自然更快。

在步骤1中，采用二分后准则函数的增幅最大为策略，每产生一个新簇，都试着将其二分并计算准则函数的增幅。然后对增幅最大的簇执行二分，重复多次直到满足算法停止条件。

### 具体实现

#### 使用方法

```java
// com.assignment.collect.serviceimpl.defectReport.DefectReportServiceImpl
if(defectReportList.size()==0)return new ResultVo<>(REQUEST_FAIL,"该任务尚未收到有效报告！");
else if(defectReportList.size()<=k){
    k=defectReportList.size();
    lists= analyzer.kmeans(k);//list.size()==k
    //                    lists=analyzer.repeatedBisection(1.0);
}else if(defectReportList.size() > k && k > 0){
    lists= analyzer.kmeans(k);//list.size()==k
}
else lists=analyzer.repeatedBisection(1.0);
```

#### 传入文档和文档标题

```java
// HanLP实现
public Document<K> addDocument(K id, List<String> document) {
    SparseVector vector = this.toVector(document);
    Document<K> d = new Document(id, vector);
    return (Document)this.documents_.put(id, d);
}
```

#### 文档向量化(统计词频)

```java
// HanLP实现
protected SparseVector toVector(List<String> wordList) {
    SparseVector vector = new SparseVector();
    Iterator var3 = wordList.iterator();

    while(var3.hasNext()) {
        String word = (String)var3.next();
        int id = this.id(word);
        Double f = vector.get(id);
        if (f == null) {
            f = 1.0D;
            vector.put(id, f);
        } else {
            vector.put(id, f + 1.0D);
        }
    }

    return vector;
}
```

#### k-means算法实现

```java
// HanLP实现
public List<Set<K>> kmeans(int nclusters) {
    if (nclusters > this.size()) {
        ConsoleLogger.logger.err("传入聚类数目%d大于文档数量%d，已纠正为文档数量\n", new Object[]{nclusters, this.size()});
        nclusters = this.size();
    }

    Cluster<K> cluster = new Cluster();
    Iterator var3 = this.documents_.values().iterator();

    while(var3.hasNext()) {
        Document<K> document = (Document)var3.next();
        cluster.add_document(document);
    }

    cluster.section(nclusters);
    this.refine_clusters(cluster.sectioned_clusters());
    List<Cluster<K>> clusters_ = new ArrayList(nclusters);
    Iterator var7 = cluster.sectioned_clusters().iterator();

    while(var7.hasNext()) {
        Cluster<K> s = (Cluster)var7.next();
        s.refresh();
        clusters_.add(s);
    }

    return this.toResult(clusters_);
}

public int size() {
    return this.documents_.size();
}

private List<Set<K>> toResult(List<Cluster<K>> clusters_) {
    List<Set<K>> result = new ArrayList(clusters_.size());
    Iterator var3 = clusters_.iterator();

    while(var3.hasNext()) {
        Cluster<K> c = (Cluster)var3.next();
        Set<K> s = new HashSet();
        Iterator var6 = c.documents_.iterator();

        while(var6.hasNext()) {
            Document<K> d = (Document)var6.next();
            s.add(d.id_);
        }

        result.add(s);
    }

    return result;
}
```

#### 重复二分聚类算法实现

```java
// HanLP实现
public List<Set<K>> repeatedBisection(int nclusters, double limit_eval) {
    if (nclusters > this.size()) {
        ConsoleLogger.logger.err("传入聚类数目%d大于文档数量%d，已纠正为文档数量\n", new Object[]{nclusters, this.size()});
        nclusters = this.size();
    }

    Cluster<K> cluster = new Cluster();
    List<Cluster<K>> clusters_ = new ArrayList(nclusters > 0 ? nclusters : 16);
    Iterator var6 = this.documents_.values().iterator();

    while(var6.hasNext()) {
        Document<K> document = (Document)var6.next();
        cluster.add_document(document);
    }

    PriorityQueue<Cluster<K>> que = new PriorityQueue();
    cluster.section(2);
    this.refine_clusters(cluster.sectioned_clusters());
    cluster.set_sectioned_gain();
    cluster.composite_vector().clear();
    que.add(cluster);

    while(!que.isEmpty() && (nclusters <= 0 || que.size() < nclusters)) {
        cluster = (Cluster)que.peek();
        if (cluster.sectioned_clusters().size() < 1 || limit_eval > 0.0D && cluster.sectioned_gain() < limit_eval) {
            break;
        }

        que.poll();
        List<Cluster<K>> sectioned = cluster.sectioned_clusters();

        Cluster c;
        for(Iterator var8 = sectioned.iterator(); var8.hasNext(); que.add(c)) {
            c = (Cluster)var8.next();
            if (c.size() >= 2) {
                c.section(2);
                this.refine_clusters(c.sectioned_clusters());
                c.set_sectioned_gain();
                if (c.sectioned_gain() < limit_eval) {
                    Iterator var10 = c.sectioned_clusters().iterator();

                    while(var10.hasNext()) {
                        Cluster<K> sub = (Cluster)var10.next();
                        sub.clear();
                    }
                }

                c.composite_vector().clear();
            }
        }
    }

    while(!que.isEmpty()) {
        clusters_.add(0, que.poll());
    }

    return this.toResult(clusters_);
}
```

## 报告能力关键词

### 基本概念

#### TextRank算法

TextRank是在Google的PageRank算法启发下，针对文本里的句子设计的权重算法，目标是自动摘要。它利用投票的原理，让每一个单词给它的邻居（术语称窗口）投赞成票，票的权重取决于自己的票数。PageRank采用矩阵迭代收敛的方式解决了这个悖论，而TextRank公式在PageRank的公式的基础上，引入了边的权值的概念，代表两个句子的相似度。但是由于此处只需要关键字，如果把一个单词视为一个句子的话，那么所有句子（单词）构成的边的权重都是0（没有交集，没有相似性），所以分子分母的权值w约掉了，算法退化为PageRank。TextRank算法公式如下。

<img src="https://ydjsir-edu.oss-cn-shanghai.aliyuncs.com/SE3/pictures/image-20220528012741016.png" alt="image-20220528012741016" style="zoom: 50%;" />

### 具体实现

#### 请求处理部分

在本项目中，业务逻辑层服务的实现在`com.assignment.collect.serviceimpl.defectReport.DefectReportServiceImpl`的`ResultVo<List<ClusterVo>> getKMeans(Integer uid, Integer k, Integer keyword)`方法中。每次前端请求关键词时，生成的结果都是现场计算的。首先对用户的报告进行聚类，而后对每一类中的报告分别生成一组关键词，最终将结果返回前端。在关键词生成的过程中调用TextRank的代码如下：

```java
for (Set set : lists) {
    //进入某个类
    Set<DefectReportVo> defectReportVos = new HashSet<>();
    List<String> groupfenci = new ArrayList<>();
    String[] descriptions = new String[0];
    List<String> groupkeywordList = new ArrayList<>();
    Map<String, Float> worldCloud = new HashMap<>();
    if (set.size() > 0) {
        for (Object id : set) {
            worldCloud = new newWordCloud().getTermAndRank(groupfenci.toString(),keyword);//带词频的关键词提取
//groupfenci是聚类完成后这一类中所有报告的分词结果，keyword代表提取topK关键词的个数k
        }
        worldCloud = new newWordCloud().getTermAndRank(groupfenci.toString(), keyword);
    }
    ClusterVo clusterVo = new ClusterVo(defectReportVos, worldCloud);
    results.add(clusterVo);
}
```

相关的类的代码位于`com.assignment.collect.prob`包下。

#### 词频统计部分

```java
/**
**输入分词后的术语列表
**输出每次术语的词频
*/
public Map<String, Float> getTermAndRank(List<Term> termList) {
    List<String> wordList = new ArrayList(termList.size());
    Iterator var3 = termList.iterator();
    //数据处理
    while(var3.hasNext()) {
        Term t = (Term)var3.next();
        if (this.shouldInclude(t)) {
            wordList.add(t.word);
        }
    }

    Map<String, Set<String>> words = new TreeMap();
    Queue<String> que = new LinkedList();
    Iterator var5 = wordList.iterator();
    //之后建立两个大小为5的窗口，每个单词将票投给它身前身后距离5以内的单词：

    while(var5.hasNext()) {
        String w = (String)var5.next();
        if (!words.containsKey(w)) {
            words.put(w, new TreeSet());
        }

        if (que.size() >= 5) {
            que.poll();
        }

        Iterator var7 = que.iterator();

        while(var7.hasNext()) {
            String qWord = (String)var7.next();
            if (!w.equals(qWord)) {
                ((Set)words.get(w)).add(qWord);
                ((Set)words.get(qWord)).add(w);
            }
        }

        que.offer(w);
    }

    Map<String, Float> score = new HashMap();
    Iterator var19 = words.entrySet().iterator();

    while(var19.hasNext()) {
        Map.Entry<String, Set<String>> entry = (Map.Entry)var19.next();
        score.put(entry.getKey(), sigMoid((float)((Set)entry.getValue()).size()));
    }
    //开始迭代投票：

    for(int i = 0; i < max_iter; ++i) {
        Map<String, Float> m = new HashMap();
        float max_diff = 0.0F;

        String key;
        for(Iterator var9 = words.entrySet().iterator(); var9.hasNext(); 
            max_diff = Math.max(max_diff, Math.abs((Float)m.get(key) - (score.get(key) == null ? 0.0F : (Float)score.get(key))))) {
            Map.Entry<String, Set<String>> entry = (Map.Entry)var9.next();
            key = (String)entry.getKey();
            Set<String> value = (Set)entry.getValue();
            m.put(key, 0.14999998F);
            Iterator var13 = value.iterator();

            while(var13.hasNext()) {
                String element = (String)var13.next();
                int size = ((Set)words.get(element)).size();
                if (!key.equals(element) && size != 0) {
                    m.put(key, (Float)m.get(key) + 0.85F / (float)size * (score.get(element) == null ? 0.0F : (Float)score.get(element)));
                }
            }
        }

        score = m;
        if (max_diff <= 0.001F) {
            break;
        }
    }

    return score;
}
```

##### 选取其中的Top K个词作为关键词

```java
private Map<String, Float> top(int size, Map<String, Float> map) {
    Map<String, Float> result = new LinkedHashMap();
    Iterator var4 = (new MaxHeap(size, new Comparator<Map.Entry<String, Float>>() {
        public int compare(Map.Entry<String, Float> o1, Map.Entry<String, Float> o2) {
            return ((Float)o1.getValue()).compareTo((Float)o2.getValue());
        }
    })).addAll(map.entrySet()).toList().iterator();

    while(var4.hasNext()) {
        Map.Entry<String, Float> entry = (Map.Entry)var4.next();
        result.put(entry.getKey(), entry.getValue());
    }

    return result;
}
```

##### 主代码

```java
public List<String> getKeywords(List<Term> termList, int size) {
    Set<Map.Entry<String, Float>> entrySet = this.top(size,this.getTermAndRank(termList)).entrySet();
    List<String> result = new ArrayList(entrySet.size());
    Iterator var5 = entrySet.iterator();

    while(var5.hasNext()) {
        Map.Entry<String, Float> entry = (Map.Entry)var5.next();
        result.add(entry.getKey());
    }

    return result;
}
```

